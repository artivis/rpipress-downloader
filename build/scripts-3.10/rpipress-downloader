#!/usr/bin/python3
# -*- coding: utf-8 -*-

import argparse
import os
import urllib.request as urlreq

import progressbar
import requests
from bs4 import BeautifulSoup

pbar = None

def progress_bar(block_num, block_size, total_size):
    global pbar
    if pbar is None:
        pbar = progressbar.ProgressBar(maxval=total_size)
        pbar.start()

    downloaded = block_num * block_size
    if downloaded < total_size:
        pbar.update(downloaded)
    else:
        pbar.finish()
        pbar = None


def is_a_clink(tag):
    return tag.name == 'a' and \
        tag.has_attr('class') and \
        tag.get('class') == ['c-link']


def downloadable_entry(tag):
    return is_a_clink(tag) and tag.has_attr('download')


def book_entry(tag):
    return is_a_clink(tag) and \
        tag.has_attr('data-category') and tag.get('data-category') == 'book'


class Magazine:
    def __init__(self, name, first_issue):
        self.name = name
        self.first_issue = first_issue


# Disabling 'wireframe' after rit closed down.
# Disabling 'custompc' after the download website isn't accessible anymore.
magazines = {
    # 'custompc' : Magazine('CustomPC', 214),
    'hackspace' : Magazine('HackSpace', 1),
    'helloworld' : Magazine('HelloWorld', 1),
    'magpi' : Magazine('MagPi', 1),
    # 'wireframe' : Magazine('Wireframe', 1),
}

parser = argparse.ArgumentParser(description="""
Download Raspberry Pi Press issues.

Freely available magazines are: HackSpace, HelloWorld and MagPi.

By default `rpipress-downloader` downloads only the latest issue of each
magazine. However you can download all issues, all books for all or some magazines.

By default, issues and books are saved respectively in
- `~/rpipress/{magazine}`
- `~/rpipress/{magazine}/Books`

or, using the snap, in
- `~/snap/rpipress-downloader/current/rpipress/{magazine}`,
- `~/snap/rpipress-downloader/current/rpipress/{magazine}/Books`.
""", formatter_class = argparse.RawTextHelpFormatter)

parser.add_argument("-a", "--all", help="Download all issues",
                    action="store_true")
parser.add_argument("-b", "--books", help="Download the magazine books",
                    action="store_true")
parser.add_argument("-m", "--magazines", help="Choose which magazine(s) to download. Defaults to all.",
                    type=str.lower, action='append', nargs='+',
                    choices=list(magazines.keys()))
parser.add_argument("-p", "--path", help="Set the download path. "
                                         "Defaults to ~/rpipress or ~/snap/rpipress-downloader/current/rpipress.",
                    type=str)
parser.add_argument("-q", "--quiet", action="store_true", help="No prints")

args = parser.parse_args()

filtered_magazines = dict()
if args.magazines:
    # Retrieve all requested magazines in flatten list
    requested_magazines = [item for sublist in args.magazines for item in sublist]

    # Remove duplicates
    requested_magazines = list(dict.fromkeys(requested_magazines))

    # Get subset
    filtered_magazines = {m: magazines[m] for m in requested_magazines}
else:
    # If none specified, get them all
    filtered_magazines = magazines

# Creating the download folder
if args.path:
    output_base_path = args.path
elif os.getenv('SNAP_USER_DATA'):
    output_base_path = os.getenv('SNAP_USER_DATA')
else:
    output_base_path = os.path.expanduser('~')

if not os.path.exists(output_base_path):
    raise FileNotFoundError(f"{output_base_path} folder does not exist!")

download = False
download_error = False
for magazine_key, magazine in filtered_magazines.items():

    output_path = os.path.join(output_base_path, 'rpipress', magazine.name)

    if not os.path.exists(output_path): os.makedirs(output_path)

    base_url = 'https://' + magazine_key + '.raspberrypi.org'

    issues_url = base_url + '/issues'

    # Find latest review number
    try:
        r = requests.get(issues_url)
        data = r.text
        soup = BeautifulSoup(data,"lxml")
        if magazine_key == "helloworld":
            last_issue = soup.find('div', class_="pk-c-detailed-hero__links").findChild("a")['href'].split('/')[-1].split("_")[0].replace("HW", "")
        else:
            last_issue = soup.find('section', class_="c-latest-issue").findChild("a")['href'].split('/')[-1]
    except:
        raise Exception('Could not find latest ' + magazine.name + ' issue number')
    else:
        if not args.quiet: print('Latest ' + magazine.name + ' issue is N°' + last_issue)

    start_issue = magazine.first_issue if args.all else int(last_issue)

    # Download issues
    for issue in range(start_issue, int(last_issue) + 1):
        try:
            issue_path = os.path.join(output_path, magazine.name + '{:02d}.pdf'.format(issue))
            if not os.path.exists(issue_path):
                if magazine_key == "helloworld":
                    label = "Download PDF - Issue " + '{:02d}'.format(issue).lstrip('0')
                    url = soup.find_all('a', attrs={"data-event-label": label})[0]['href']
                    show_progress = progress_bar if not args.quiet else None
                    urlreq.urlretrieve(url, issue_path, show_progress)
                    download = True
                else:
                    r = requests.get(issues_url + '/{:02d}/pdf'.format(issue))
                    data = r.text
                    soup = BeautifulSoup(data, "lxml")
                    tag = soup.find(downloadable_entry)
                    if not tag:
                        r = requests.get(issues_url + '/{:02d}/pdf/download'.format(issue))
                        data = r.text
                        soup = BeautifulSoup(data, "lxml")
                        tag = soup.find(is_a_clink, string='click here to get your free PDF')
                        tag['href'] = base_url + tag['href']
                    show_progress = progress_bar if not args.quiet else None
                    urlreq.urlretrieve(tag['href'], issue_path, show_progress)
                    download = True
            else:
                continue
        except:
            if not args.quiet: print('ERROR: There was an error downloading ' + magazine.name + ' N°{:02d}'.format(issue))
            download_error = True
        else:
            if not args.quiet: print(magazine.name + ' N°{:02d} downloaded!'.format(issue))

    if args.books:

        books = {}

        # Find all books
        try:
            r = requests.get(base_url + '/books')
            data = r.text
            soup = BeautifulSoup(data, "lxml")
            tags = soup.find_all(book_entry)
        except:
            if not args.quiet: print('ERROR: There was an error retrieving the book list')
        else:
            for tag in tags:
                books[tag['data-label']] = tag['href']

            if books:
                output_path = os.path.join(output_path, 'Books')
                if not os.path.exists(output_path): os.makedirs(output_path)

                # Download each book if not already there
                for book_name, book_href in books.items():
                    try:
                        book_path = os.path.join(output_path, book_name + '.pdf')
                        if not os.path.exists(book_path):
                            r = requests.get(base_url + book_href + '/pdf')
                            data = r.text
                            soup = BeautifulSoup(data, "lxml")
                            tag = soup.find(downloadable_entry)
                            if not tag:
                                r = requests.get(base_url + book_href + '/pdf/download')
                                data = r.text
                                soup = BeautifulSoup(data, "lxml")
                                tag = soup.find(is_a_clink, string='click here to get your free PDF')
                                tag['href'] = base_url + tag['href']
                            show_progress = progress_bar if not args.quiet else None
                            urlreq.urlretrieve(tag['href'], book_path, show_progress)
                            download = True
                        else:
                            continue
                    except:
                        if not args.quiet: print('ERROR: There was an error downloading ' + magazine.name + ' book ' + book_name)
                        download_error = True
                    else:
                        if not args.quiet: print(magazine.name + ' book \'' + book_name + '\' downloaded!')

if not args.quiet:
    if not download and not download_error: print('You are up to date.')
    print('Your favorite magazines are waiting for you in ' +
          'file://' + os.path.join(output_base_path, 'rpipress'))
