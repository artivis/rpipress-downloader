#!/usr/bin/env python3
import argparse
import os
import urllib.request as urlreq

import progressbar
import requests
from bs4 import BeautifulSoup

pbar = None

def progress_bar(block_num, block_size, total_size):
    global pbar
    if pbar is None:
        pbar = progressbar.ProgressBar(maxval=total_size)
        pbar.start()

    downloaded = block_num * block_size
    if downloaded < total_size:
        pbar.update(downloaded)
    else:
        pbar.finish()
        pbar = None


def is_a_clink(tag):
    return tag.name == 'a' and \
        tag.has_attr('class') and \
        tag.get('class') == ['c-link']


def downloadable_entry(tag):
    return is_a_clink(tag) and tag.has_attr('download')


def book_entry(tag):
    return is_a_clink(tag) and \
        tag.has_attr('data-category') and tag.get('data-category') == 'book'

class colors:
    OK = '\033[92m'
    ERROR = '\033[91m'
    FADE = '\033[2m'
    ENDC = '\033[0m'

class log:
    ERROR = colors.ERROR + "ERROR:" + colors.ENDC

class Magazine:
    def __init__(self, name, first_issue):
        self.name = name
        self.first_issue = first_issue

magazines = {
    'hackspace' : Magazine('HackSpace', 1),
    'helloworld' : Magazine('HelloWorld', 1),
    'magpi' : Magazine('MagPi', 1),
}

parser = argparse.ArgumentParser(description="""
Download Raspberry Pi Press issues.

Freely available magazines are: HackSpace, HelloWorld and MagPi.

By default `rpipress-downloader` downloads only the latest issue of each
magazine. However you can download all issues, all books for all or some magazines.

By default, issues and books are saved respectively in
- `~/rpipress/{magazine}`
- `~/rpipress/{magazine}/Books`

or, using the snap, in
- `~/snap/rpipress-downloader/current/rpipress/{magazine}`,
- `~/snap/rpipress-downloader/current/rpipress/{magazine}/Books`.
""", formatter_class = argparse.RawTextHelpFormatter)

parser.add_argument("-a", "--all", help="Download all issues",
                    action="store_true")
parser.add_argument("-b", "--books", help="Download the magazine books",
                    action="store_true")
parser.add_argument("-m", "--magazines", help="Choose which magazine(s) to download. Defaults to all.",
                    type=str.lower, action='append', nargs='+',
                    choices=list(magazines.keys()))
parser.add_argument("-p", "--path", help="Set the download path. "
                    "Defaults to ~/rpipress or ~/snap/rpipress-downloader/current/rpipress.",
                    type=str)
parser.add_argument("-q", "--quiet", action="store_true", help="No prints")

args = parser.parse_args()

filtered_magazines = dict()
if args.magazines:
    # Retrieve all requested magazines in flatten list
    requested_magazines = [item for sublist in args.magazines for item in sublist]

    # Remove duplicates
    requested_magazines = list(dict.fromkeys(requested_magazines))

    # Get subset
    filtered_magazines = {m: magazines[m] for m in requested_magazines}
else:
    # If none specified, get them all
    filtered_magazines = magazines

# Creating the download folder
if args.path:
    output_base_path = args.path
elif os.getenv('SNAP_USER_DATA/rpipress'):
    output_base_path = os.getenv('SNAP_USER_DATA/rpipress')
else:
    output_base_path = os.path.expanduser('~/rpipress')

if not os.path.exists(output_base_path):
    os.makedirs(output_base_path)
    # raise  FileNotFoundError(f"{output_base_path} folder does not exist!")

download = False
download_error = False
for magazine_key, magazine in filtered_magazines.items():

    output_path = os.path.join(output_base_path, magazine.name)

    if not os.path.exists(output_path): os.makedirs(output_path)

    base_url = 'https://' + magazine_key + '.raspberrypi.org'

    issues_url = base_url + '/issues'

    # Find latest review number
    try:
        r = requests.get(issues_url)
        data = r.text
        soup = BeautifulSoup(data,"lxml")
        if magazine_key == "helloworld":
            last_issue = soup.find('div', class_="pk-c-detailed-hero__links").findChild("a")['data-event-label'].split("Issue ")[-1]
        else:
            last_issue = soup.find('section', class_="c-latest-issue").findChild("a")['href'].split('/')[-1]
    except:
        raise Exception('Could not find latest ' + magazine.name + ' issue number')
    else:
        if not args.quiet:
            print('Latest {} issue is N°{:02d}'.format(magazine.name, int(last_issue)) )

    start_issue = int(last_issue)-3 if args.all else int(last_issue)

    # Download issues
    for issue in range(start_issue, int(last_issue) + 1):
        issue_path = os.path.join(output_path, magazine.name + '{:02d}.pdf'.format(issue))
        if not os.path.exists(issue_path):
            try:
                if magazine_key == "helloworld":
                    label = "Download PDF - Issue {}".format(issue)
                    url = soup.find_all('a', attrs={"data-event-label": label})[0]['href']
                else:
                    r = requests.get(issues_url + '/{:02d}/pdf/download'.format(issue))
                    data = r.text
                    soup = BeautifulSoup(data, "lxml")
                    tag = soup.find(is_a_clink, string='click here to get your free PDF')
                    url = base_url + tag['href']

                show_progress = progress_bar if not args.quiet else None
                urlreq.urlretrieve(url, issue_path, show_progress)
                download = True
            except:
                if not args.quiet:
                    print(
                        colors.FADE
                        + 'You may need to wait for the free download.\n'
                        + colors.ENDC
                    )
                download_error = True
            else:
                if not args.quiet:
                    print(
                        colors.OK
                        + '{} N°{:02d} downloaded!'.format(magazine.name, issue)
                        + colors.ENDC
                    )
        
        
    if args.books:

        books = {}

        # Find all books
        try:
            r = requests.get(base_url + '/books')
            data = r.text
            soup = BeautifulSoup(data, "lxml")
            tags = soup.find_all(book_entry)
        except:
            if not args.quiet:
                print(log.ERROR + 'There was an error retrieving the book list')
        else:
            for tag in tags:
                books[tag['data-label']] = tag['href']

            if books:
                output_path = os.path.join(output_path, 'Books')
                if not os.path.exists(output_path): os.makedirs(output_path)

                # Download each book if not already there
                for book_name, book_href in books.items():
                    book_path = os.path.join(output_path, book_name + '.pdf')
                    
                    if not os.path.exists(book_path):
                        if not args.quiet:
                            print('Latest {} book is \'{}\''.format(magazine.name, book_name))
                        try:
                            r = requests.get(base_url + book_href + '/pdf/download')
                            data = r.text
                            soup = BeautifulSoup(data, "lxml")
                            tag = soup.find(is_a_clink, string='click here to get your free PDF')
                            url = base_url + tag['href']
                            show_progress = progress_bar if not args.quiet else None
                            urlreq.urlretrieve(url, book_path, show_progress)
                            download = True

                        except:
                            if not args.quiet:
                                print(
                                    colors.FADE
                                    + 'You may need to wait for the free download.\n'
                                    + colors.ENDC
                                )
                            download_error = True
                        else:
                            if not args.quiet:
                                print(
                                    colors.OK
                                    +'{} book \'{}\' downloaded!'.format(magazine.name, book_name)
                                    + colors.ENDC
                                )


if not args.quiet:
    if not download and not download_error:
        print(
            colors.OK 
            + 'You are up to date.\n' 
            + colors.ENDC
        )
    print(
        colors.OK
        + 'Your favorite magazines are waiting for you in file://'
        + os.path.join(output_base_path)
        + colors.ENDC
    )
